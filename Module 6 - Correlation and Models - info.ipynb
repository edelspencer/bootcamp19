{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Module 6 - Correlation & Linear Regression"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Bring in our usual libraries. . . . "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# WHY ARE WE EVEN DOING THIS?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The overall purpose of conducting regression analyses is to discover the RELATIONSHIP between variables of interest. As an added bonus, regression can show us the relationships between variables while simultaneously controlling for the effects of other variables - for example:\n\nI want to figure out the relationship between age and hair loss. I have a feeling that these two variables have a POSITIVE LINEAR relationship (as AGE increases, HAIR LOSS increases). However, I know it can't be this simple. . . .  What if the relationship between age and hair loss is influenced by gender, geographic location, medical history, family history, hat preference, and hair color... the list is endless! Therefore, to figure out the relationship between age and hair loss - we have to CONTROL for the potential influence that all these other factors might have on that relationship. We are able to do this with regression analyses . . .  at the end of the analyses we will be able to more confidently say: \n\nthe relationship between age and hair loss is this, and I know this because I thought about all the other factors that might influence that relationship, and I controlled for them - so the results I'm seeing are more accurate. \n\nThe entire purpose of regression is to tease out the relationships between variables, but before we get to interpret our results, there are a few things we need to do.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Correlation"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "Location = \"datasets/gradedata.csv\"\ndf = pd.read_csv(Location)\n\ndf.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "     fname     lname  gender  age  exercise  hours  grade  \\\n0   Marcia      Pugh  female   17         3     10   82.4   \n1   Kadeem  Morrison    male   18         4      4   78.2   \n2     Nash    Powell    male   18         5      9   79.3   \n3  Noelani    Wagner  female   14         2      7   83.2   \n4  Noelani    Cherry  female   18         4     15   87.4   \n\n                                    address  \n0   9253 Richardson Road, Matawan, NJ 07747  \n1          33 Spring Dr., Taunton, MA 02780  \n2          41 Hill Avenue, Mentor, OH 44060  \n3        8839 Marshall St., Miami, FL 33125  \n4  8304 Charles Rd., Lewis Center, OH 43035  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>lname</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>exercise</th>\n      <th>hours</th>\n      <th>grade</th>\n      <th>address</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Marcia</td>\n      <td>Pugh</td>\n      <td>female</td>\n      <td>17</td>\n      <td>3</td>\n      <td>10</td>\n      <td>82.4</td>\n      <td>9253 Richardson Road, Matawan, NJ 07747</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kadeem</td>\n      <td>Morrison</td>\n      <td>male</td>\n      <td>18</td>\n      <td>4</td>\n      <td>4</td>\n      <td>78.2</td>\n      <td>33 Spring Dr., Taunton, MA 02780</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nash</td>\n      <td>Powell</td>\n      <td>male</td>\n      <td>18</td>\n      <td>5</td>\n      <td>9</td>\n      <td>79.3</td>\n      <td>41 Hill Avenue, Mentor, OH 44060</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Noelani</td>\n      <td>Wagner</td>\n      <td>female</td>\n      <td>14</td>\n      <td>2</td>\n      <td>7</td>\n      <td>83.2</td>\n      <td>8839 Marshall St., Miami, FL 33125</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Noelani</td>\n      <td>Cherry</td>\n      <td>female</td>\n      <td>18</td>\n      <td>4</td>\n      <td>15</td>\n      <td>87.4</td>\n      <td>8304 Charles Rd., Lewis Center, OH 43035</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To help us determine which variables are best to include in our regression, we want to figure out which variables have at least some kind of relationship to each other. This is because regressions work the best when we don't overrun them with meaningless variables - we only want to include the variables that have meaning to our dependent variable (refresher below). To do this, we run a CORRELATION. This analysis shows us the linear relationship between variables. Remember, only numeric variables have meaningful output for a correlation. \n\nVariable Refresher:\n\n<b>INDEPENDENT VARIABLE:</b> The independent variable is the variable whose change isn’t affected by any other variable in the experiment. Either the scientist has to change the independent variable herself or it changes on its own; nothing else in the experiment affects or changes it. Two examples of common independent variables are age and time. There’s nothing you or anything else can do to speed up or slow down time or increase or decrease age. They’re independent of everything else.\n\n<b>DEPENDENT VARIABLE:</b> The dependent variable is what is being studied and measured in the experiment. It’s what changes as a result of the changes to the independent variable. An example of a dependent variable is how tall you are at different ages. The dependent variable (height) depends on the independent variable (age).\n"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.corr()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "               age  exercise     hours     grade\nage       1.000000 -0.003643 -0.017467 -0.007580\nexercise -0.003643  1.000000  0.021105  0.161286\nhours    -0.017467  0.021105  1.000000  0.801955\ngrade    -0.007580  0.161286  0.801955  1.000000",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>exercise</th>\n      <th>hours</th>\n      <th>grade</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>age</th>\n      <td>1.000000</td>\n      <td>-0.003643</td>\n      <td>-0.017467</td>\n      <td>-0.007580</td>\n    </tr>\n    <tr>\n      <th>exercise</th>\n      <td>-0.003643</td>\n      <td>1.000000</td>\n      <td>0.021105</td>\n      <td>0.161286</td>\n    </tr>\n    <tr>\n      <th>hours</th>\n      <td>-0.017467</td>\n      <td>0.021105</td>\n      <td>1.000000</td>\n      <td>0.801955</td>\n    </tr>\n    <tr>\n      <th>grade</th>\n      <td>-0.007580</td>\n      <td>0.161286</td>\n      <td>0.801955</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The results of a correlation give us a numeric estimation of the magnitude of the linear realtionship between variables. The +/- of the number doesn't matter when determining the strength of the relationship, the actual number is what matters when determining the strength of the relationship. The larger the number (regardless if the number is negative), the greater the relationship. If a number is negative, it means that the relationship is negative (or inverse) - when one value goes up, the other goes down. When the number is positive, it means that the relationship is positive - the values move in the same direction - when one goes up the other goes up, when one goes down the other goes down. \n\nOur output shows us:\n\n* Age doesn't have a strong relationship with any variables\n\n* Exercise has weak, positive relationships with hours and grade. \n\n* Hours has a strong, positive relationship with grade. \n\n\nWhen looking at this, it would seem that Age doesn't have much to do with the other variables. But since this is an exercise, we're still going to include it in our regression. However, if you were in the position of having to determine which variables to eliminate, we want to eliminate the ones that have the weakest relationship. \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Linear Regression"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we know which variables we want to include in our regression, we have to set up the regression analysis and write our code. \n\nFirst we bring in the library needed to run the regression. . . "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import statsmodels.formula.api as smf",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we write out the regression equation. This is where we specify which variable we are considering to be dependent/independent. \n\nIn the below equation GRADE is the dependent variable. AGE, EXERCISE, HOURS are the independent variables. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "result = smf.ols('grade ~ age + exercise + hours', data=df).fit()",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To see the results of the analysis we use the code below to get the summary. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "result.summary()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  grade   R-squared:                       0.664\nModel:                            OLS   Adj. R-squared:                  0.664\nMethod:                 Least Squares   F-statistic:                     1315.\nDate:                Tue, 05 Mar 2019   Prob (F-statistic):               0.00\nTime:                        20:42:40   Log-Likelihood:                -6300.7\nNo. Observations:                2000   AIC:                         1.261e+04\nDf Residuals:                    1996   BIC:                         1.263e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     57.8704      1.321     43.804      0.000      55.279      60.461\nage            0.0397      0.075      0.532      0.595      -0.107       0.186\nexercise       0.9893      0.089     11.131      0.000       0.815       1.164\nhours          1.9165      0.031     61.564      0.000       1.855       1.978\n==============================================================================\nOmnibus:                      321.187   Durbin-Watson:                   2.047\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2196.187\nSkew:                          -0.567   Prob(JB):                         0.00\nKurtosis:                       8.007   Cond. No.                         213.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>grade</td>      <th>  R-squared:         </th> <td>   0.664</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.664</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1315.</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 05 Mar 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>20:42:40</td>     <th>  Log-Likelihood:    </th> <td> -6300.7</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  2000</td>      <th>  AIC:               </th> <td>1.261e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  1996</td>      <th>  BIC:               </th> <td>1.263e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   57.8704</td> <td>    1.321</td> <td>   43.804</td> <td> 0.000</td> <td>   55.279</td> <td>   60.461</td>\n</tr>\n<tr>\n  <th>age</th>       <td>    0.0397</td> <td>    0.075</td> <td>    0.532</td> <td> 0.595</td> <td>   -0.107</td> <td>    0.186</td>\n</tr>\n<tr>\n  <th>exercise</th>  <td>    0.9893</td> <td>    0.089</td> <td>   11.131</td> <td> 0.000</td> <td>    0.815</td> <td>    1.164</td>\n</tr>\n<tr>\n  <th>hours</th>     <td>    1.9165</td> <td>    0.031</td> <td>   61.564</td> <td> 0.000</td> <td>    1.855</td> <td>    1.978</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>321.187</td> <th>  Durbin-Watson:     </th> <td>   2.047</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2196.187</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.567</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 8.007</td>  <th>  Cond. No.          </th> <td>    213.</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Okay, now we are in business!\n\nThere is A LOT of information in these results. Let's break it down with just the most important aspects:\n\n<b>Adj. R-squared</b>\n* This number shows the amount of variation in the data that is explained by our regression equation. The higher this number, the better the model (regression equation/analysis) fits. When you are running multiple models (and swamping out variables), you typically want to go with the model that has the highest Adj. R-squared!\n\n<b>Intercept</b>\n* This is showing what the value of the dependent variable would be if all the independent variables (IV) are 0. Sometimes this makes sense, sometimes this doesn't. In this example, the dependent variable (DV) is Grade - so when Age (that would be weird), Exercise, and Hours of Studying are all 0, your starting grade would be around 57.87. \n\n<b>Coef</b>\n* The regression coefficients represent the mean change in the DV for a one unit change in the IV, while controlling for the other IV's. You can think of the coefficients as slopes. For example - every one year increase in AGE, GRADE increases by 0.0397. \n\n<b>p-value</b>\n* This is the crème de la crème of the regression output. The p-value - shown in the output as \"P>|t|\" - is letting you know if there is a statistically significant relationship being shown in the regression. The p-value is the percent chance that the results you see are just by chance, and there is not actually a true relationship. Therefore, we want the *smallest* percent chance - and typically a p-value <= 0.005 is considered a statistically significant result. \n\nFrom our results, we can see that the Adj R-Squared is 0.664 (not bad), and that intercept, exercise, and hours are the statistically significant variables. This makes sense - since we saw that Age didn't have a strong correlation with the other variables. \n\nAt this point, we want to work with our model to see if we can increase the fit... there are a few things to try at this point. \n\nFirst, we want to try the model without the non-significant variable(s). In this case, that's AGE. Let's remove age from our equation and see how the model fit changes."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With age, exercise, and hours being 0, your starting grade is likely to be around 57.87"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#remove age from regression, not very correlated\nresult = smf.ols('grade ~ exercise + hours', data=df).fit()\nresult.summary()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  grade   R-squared:                       0.664\nModel:                            OLS   Adj. R-squared:                  0.664\nMethod:                 Least Squares   F-statistic:                     1973.\nDate:                Tue, 05 Mar 2019   Prob (F-statistic):               0.00\nTime:                        21:03:01   Log-Likelihood:                -6300.8\nNo. Observations:                2000   AIC:                         1.261e+04\nDf Residuals:                    1997   BIC:                         1.262e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     58.5316      0.447    130.828      0.000      57.654      59.409\nexercise       0.9892      0.089     11.131      0.000       0.815       1.163\nhours          1.9162      0.031     61.575      0.000       1.855       1.977\n==============================================================================\nOmnibus:                      318.721   Durbin-Watson:                   2.048\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2158.000\nSkew:                          -0.564   Prob(JB):                         0.00\nKurtosis:                       7.962   Cond. No.                         43.2\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>grade</td>      <th>  R-squared:         </th> <td>   0.664</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.664</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1973.</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 05 Mar 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>21:03:01</td>     <th>  Log-Likelihood:    </th> <td> -6300.8</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  2000</td>      <th>  AIC:               </th> <td>1.261e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  1997</td>      <th>  BIC:               </th> <td>1.262e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   58.5316</td> <td>    0.447</td> <td>  130.828</td> <td> 0.000</td> <td>   57.654</td> <td>   59.409</td>\n</tr>\n<tr>\n  <th>exercise</th>  <td>    0.9892</td> <td>    0.089</td> <td>   11.131</td> <td> 0.000</td> <td>    0.815</td> <td>    1.163</td>\n</tr>\n<tr>\n  <th>hours</th>     <td>    1.9162</td> <td>    0.031</td> <td>   61.575</td> <td> 0.000</td> <td>    1.855</td> <td>    1.977</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>318.721</td> <th>  Durbin-Watson:     </th> <td>   2.048</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2158.000</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.564</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 7.962</td>  <th>  Cond. No.          </th> <td>    43.2</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So we took AGE out of our regression - and the Adj R-squared didn't change, and the significance of our IV's also didn't change. This isn't a bad thing, but it means that AGE may not be influencing the model much. \n\nEven if it makes sense overall - we want to also try a model without the \"Intercept\" variable. To do this, we simply add a \"-1\" to the end of our regression equation in our code. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#set coefficient to 0\nresult = smf.ols(formula='grade ~ age + exercise + hours - 1', data=df).fit()\nresult.summary()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  grade   R-squared:                       0.991\nModel:                            OLS   Adj. R-squared:                  0.991\nMethod:                 Least Squares   F-statistic:                 7.284e+04\nDate:                Tue, 05 Mar 2019   Prob (F-statistic):               0.00\nTime:                        21:05:00   Log-Likelihood:                -6974.3\nNo. Observations:                2000   AIC:                         1.395e+04\nDf Residuals:                    1997   BIC:                         1.397e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nage            3.1129      0.035     88.030      0.000       3.044       3.182\nexercise       1.7659      0.122     14.482      0.000       1.527       2.005\nhours          2.2860      0.042     54.486      0.000       2.204       2.368\n==============================================================================\nOmnibus:                      131.221   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              403.367\nSkew:                          -0.301   Prob(JB):                     2.57e-88\nKurtosis:                       5.116   Cond. No.                         14.2\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>grade</td>      <th>  R-squared:         </th> <td>   0.991</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.991</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.284e+04</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 05 Mar 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>21:05:00</td>     <th>  Log-Likelihood:    </th> <td> -6974.3</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  2000</td>      <th>  AIC:               </th> <td>1.395e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  1997</td>      <th>  BIC:               </th> <td>1.397e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>age</th>      <td>    3.1129</td> <td>    0.035</td> <td>   88.030</td> <td> 0.000</td> <td>    3.044</td> <td>    3.182</td>\n</tr>\n<tr>\n  <th>exercise</th> <td>    1.7659</td> <td>    0.122</td> <td>   14.482</td> <td> 0.000</td> <td>    1.527</td> <td>    2.005</td>\n</tr>\n<tr>\n  <th>hours</th>    <td>    2.2860</td> <td>    0.042</td> <td>   54.486</td> <td> 0.000</td> <td>    2.204</td> <td>    2.368</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>131.221</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 403.367</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.301</td>  <th>  Prob(JB):          </th> <td>2.57e-88</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.116</td>  <th>  Cond. No.          </th> <td>    14.2</td>\n</tr>\n</table>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "HUGE change in that Adj R-squared! And AGE suddenly decided to show up for the party!\n\nSince the Adj R-Squared is much higher, we can safely assume that this model fits our data better than the previous models. Using this model, we can now make some assumptions about our results:\n\n<b> AGE </b>\n\nFor every one year increase in AGE, GRADE increases by 3.1129.\n\n<b> EXERCISE </b>\n\nFor every one hour increase in EXERCISE time, GRADE increases by 1.77. \n\n<b> HOURS </b>\n\nFor every additional HOUR spent studying, GRADE increases by 2.29. \n\n\n*Maybe our data is showing us that the older, more studious and more health conscious students are getting better grades.* \n\n<b>*side note*</b> if any of our coefficients were negative, this would indicate that the relationship with the DV is inverse, i.e. if the coef for AGE was \"-3.11\" -- we could interpret this as: For every one year increase of AGE, GRADE decreases by 3.11. \n\nNOW GO FORTH AND TRY THE EXERCISES!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Your Turn\n\nRun a correlation and regresssion on the dataset below. What can you tell from the data?"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Location = \"datasets/datasets/tamiami.csv\"\n\ndf = pd.read_csv(Location)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "columns = ['location', 'sales', 'employees', 'restaurants', 'foodcarts', 'price']\n\n#change column names for readability\ndf.columns = columns\ndf.head()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}